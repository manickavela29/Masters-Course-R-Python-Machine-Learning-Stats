Matrix Vector Multiply (version 1):
Let Matrix M be available in M.txt in hdfs. Let V be available in V.txt, also in hdfs.
Using MapReduce find Mv and store the output in Mv.txt, in hdfs.

M = 
1 2 -3 -1
-4 5 6 -3
7 -8 9 -7

M.txt is as follows: Format is (i,j,Mij)
1,1,1
1,2,2
1,3,-3
1,4,-1
2,1,-4
2,2,5
2,3,6
2,4,-3
3,1,7
3,2,-8
3,3,9
3,4,-7

V =
-1
2
3
-4

V.txt is as follows: Format is (j,Vj)
1,-1
2,2
3,3
4,-4

This version of Matrix Vector multiply works only for 3x4 matrices and 4x1 vectors.
But keep variables like m and n, so that it can be easily changed for any mxn.

Hints:
1. Though V.txt is in hdfs, it must be read in each Map task as each task needs the entire Vector V. Look at the setup() method in the Mapper class which is useful to setup one time tasks before any Map tasks begin.
2. The following code could help in reading files from HDFS (in your Map class):
	Path pt=new Path("hdfs:/path/to/file");//Location of file in HDFS
        FileSystem fs = FileSystem.get(new Configuration());
        BufferedReader br=new BufferedReader(new InputStreamReader(fs.open(pt)));
        String line;
        line=br.readLine();
        while (line != null){
            System.out.println(line);
            line=br.readLine();
        }
3. Load the Vector into a static variable and use it across all Map tasks.
4. The output key class and output value class specified in the main java file indicate the key and value output from Map and Reduce. But if it differs, look at j.setMapOutputKeyClass(...) and j.setMapOutputValueClass(...), where j is a Job instance.
5. Whenever a Value class is an user implemented class, then it should implement the Writable interface.
6. Whenever a Key class is an user implemented class, then it should implement the WritableComparable<> at org.apache.hadoop.io.WritableComparable



[(1,1,1),
(1,2,2),
(1,3,-3),
(1,4,-1),
(2,1,-4),
(2,2,5),
(2,3,6),
(2,4,-3),
(3,1,7),
(3,2,-8),
(3,3,9),
(3,4,-7)]


[-1,2,3,-4]